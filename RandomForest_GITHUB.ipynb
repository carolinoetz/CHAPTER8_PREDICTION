{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c60a4e02-1192-4160-8947-0dd060766175",
   "metadata": {},
   "source": [
    "# Random Forest Regression on HMM Sleep Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1572d2c0-ada2-4903-b273-4dd27f09c703",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate, GridSearchCV, train_test_split, GroupKFold\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, r2_score, mean_absolute_error, explained_variance_score\n",
    "from sklearn.linear_model import ElasticNet, LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import KNNImputer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.model_selection import GridSearchCV, GroupKFold\n",
    "from math import sqrt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138cb6ff-04fe-4748-9ea2-eefa4518c1c3",
   "metadata": {},
   "source": [
    "## Functions\n",
    "**Create scorer's for the models to assess their fit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5fe688-a5c8-40ac-9dcc-d643a3c3a955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom RMSE scorer\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "# Custom calibration slope scorer\n",
    "def calibration_slope(y_true, y_pred):\n",
    "    model = LinearRegression()\n",
    "    model.fit(y_pred.reshape(-1, 1), y_true)\n",
    "    return model.coef_[0]\n",
    "\n",
    "# Custom calibration intercept scorer\n",
    "def calibration_intercept(y_true, y_pred):\n",
    "    model = LinearRegression()\n",
    "    model.fit(y_pred.reshape(-1, 1), y_true)\n",
    "    return model.intercept_\n",
    "\n",
    "# Define the scorer's dictionary\n",
    "scorers = {\n",
    "    'rmse': make_scorer(rmse),\n",
    "    'r2': make_scorer(r2_score),\n",
    "    'mae': make_scorer(mean_absolute_error),\n",
    "    'explained_variance': make_scorer(explained_variance_score),\n",
    "    'calibration_slope': make_scorer(calibration_slope),\n",
    "    'calibration_intercept': make_scorer(calibration_intercept)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8837af8d-c9da-47db-9dc9-e8804c7f1fc6",
   "metadata": {},
   "source": [
    "## Dataset up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db7a947-b1e3-4f05-83e6-d5b7dd957284",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sleep = pd.read_csv(\"Sleep_total_data_scaled_probs.csv\")\n",
    "df_activity = pd.read_csv(\"Act_total_data_scaled_probs.csv\")\n",
    "df_predictors = pd.read_csv(\"df_predictors.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fac490-6bb5-4b81-9839-ee3bc821d4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SLEEP!!!! \n",
    "df_sleep = df_sleep.rename(columns={'hmm': 'HMM_State_Sleep', \n",
    "                                    'State_Prob_1': 'State_Prob_1_Sleep',\n",
    "                                    'State_Prob_2': 'State_Prob_2_Sleep',\n",
    "                                    'State_Prob_3': 'State_Prob_3_Sleep', \n",
    "                                    'State_Prob_4': 'State_Prob_4_Sleep'})\n",
    "\n",
    "df_sleep = df_sleep[['p_id','timepoint', 'IDS_TOTAL', 'HMM_State_Sleep','State_Prob_1_Sleep', 'State_Prob_2_Sleep',\n",
    "       'State_Prob_3_Sleep','State_Prob_4_Sleep', 'sleep_day',\n",
    "       'total_sleep_time_mean', 'total_sleep_time_sd', 'awake_pct_mean',\n",
    "       'sleep_onset_mean', 'sleep_onset_sd', 'sleep_offset_mean',\n",
    "       'sleep_offset_sd', 'sleep_efficiency_mean', 'sleep_efficiency_sd']]\n",
    "\n",
    "# ACTIVITY !!!!\n",
    "df_activity = df_activity.rename(columns={'hmm': 'HMM_State_Act', \n",
    "                                    'State_Prob_1': 'State_Prob_1_Act',\n",
    "                                    'State_Prob_2': 'State_Prob_2_Act',\n",
    "                                    'State_Prob_3': 'State_Prob_3_Act'})\n",
    "\n",
    "df_activity = df_activity[['p_id','timepoint','HMM_State_Act', 'State_Prob_1_Act', 'State_Prob_2_Act',\n",
    "                           'State_Prob_3_Act','act_day',\n",
    "                           'Sedentary_time_mean',\n",
    "                           'Light_activity_mean', 'Moderate_activity_mean',\n",
    "                           'Vigorous_activity_mean', 'Nighttime_activity_mean',\n",
    "                           'Total_Daily_Calories_mean', 'Sedentary_time_sd', 'Light_activity_sd',\n",
    "                           'Moderate_activity_sd', 'Vigorous_activity_sd', 'Nighttime_activity_sd',\n",
    "                           'Total_Daily_Calories_sd']]              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bbdfd5-9b5a-4c13-876d-eb98a2df6ba0",
   "metadata": {},
   "source": [
    "**Sleep + Baseline (predictors) dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9c5fc5-78a9-4ca9-baf4-ca0372cd8ce4",
   "metadata": {},
   "source": [
    "Gender: Female = 0; Male =1 \n",
    "Deciding against using dummy variables & using the probabilites instead -- if I was to use the cluster memberships again MAKE SURE TO change the code to not include NAs as zeros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfa2345-c4da-4525-9f5d-0f4cf740f506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge sleep & act data \n",
    "df_sleep_pre = pd.merge(df_sleep, df_predictors, on = ['p_id', 'timepoint'], how= 'inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa64d41-3d5a-4a84-9e6f-c8832f032c14",
   "metadata": {},
   "source": [
    "**Sleep + Activity + Baseline (predictors)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26b91e0-8c9c-4f1b-9c68-7d60dcf14f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sleep_act = pd.merge(df_activity, df_sleep, on = ['p_id', 'timepoint'], how= 'inner')\n",
    "df_sleep_act_pre = pd.merge(df_sleep_act, df_predictors, on = ['p_id', 'timepoint'], how= 'inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d82d23d-5fb3-4102-b50b-f1419775a303",
   "metadata": {},
   "source": [
    "### The two final samples \n",
    "**Sample 1 = sleep only**\n",
    "\n",
    "**Sample 2 = sleep + activity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7a35d2-32e9-413b-bc9c-45226546a50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_1 = df_sleep_pre\n",
    "sample_2 = df_sleep_act_pre"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbed864-ead1-490d-87e1-ae62331f3c03",
   "metadata": {},
   "source": [
    "### Remove where timepoint == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5566b87d-04fb-4bfd-9209-4a87ffe58795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where 'Score' equals 0\n",
    "sample_1 = sample_1[sample_1['timepoint'] != 0]\n",
    "sample_2 = sample_2[sample_2['timepoint'] != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc3de32-167f-443a-8ca9-7be5371252f0",
   "metadata": {},
   "source": [
    "**Exploring missingness**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab5f073-c5e5-4879-8e8a-c2c6e5251c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the display option to show all rows\n",
    "pd.set_option('display.max_rows', None)\n",
    "missing_values_df = sample_1.isna().sum().reset_index()  #<--- CHANGE HERE\n",
    "non_missing_values_df = sample_1.notna().sum().reset_index(drop=True) #<--- CHANGE HERE\n",
    "na_df = missing_values_df\n",
    "na_df.columns = ['Variable', 'Missing Values']\n",
    "na_df['Non-missing Values'] = non_missing_values_df\n",
    "print(na_df)\n",
    "pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a7b3ec-e277-4446-a461-0ae56a332271",
   "metadata": {},
   "source": [
    "# Random Forest - Nested CV "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92812a7e-4930-4c9f-b27a-486b4cf4d9ed",
   "metadata": {},
   "source": [
    "Here we have an outer and an inner loop. The outer loop is used to evaluate the model & the inner loop to tune the hyperparameters. \n",
    "1. Outer Loop: Use GroupKFold for cross-validation (same as above)\n",
    "2. Inner Loop: Use GridSearchCV within each of these folds from the outer loop to tune the hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d17d7fd-5e3b-4354-8708-2cb0ee34c1a2",
   "metadata": {},
   "source": [
    "#### Creating feature set dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e263788-9ef0-4c0c-9096-e0f44e35bf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chara = ['Baseline_IDS_TOTAL',\n",
    "            'age_all', 'gender_num', 'mh_family_depr___0', 'mh_family_depr___1', 'mh_family_depr___2', 'LIFETIME_TRAUMA',\n",
    "            'Mental_comorbidity', 'Physical_comorbidity'\n",
    "           ]\n",
    "\n",
    "chara_wo_baselineids = [\n",
    "            'age_all', 'gender_num', 'mh_family_depr___0', 'mh_family_depr___1', 'mh_family_depr___2', 'LIFETIME_TRAUMA',\n",
    "            'Mental_comorbidity', 'Physical_comorbidity'\n",
    "           ]\n",
    "\n",
    "sleep_cluster = ['State_Prob_1_Sleep', 'State_Prob_2_Sleep', 'State_Prob_3_Sleep', 'State_Prob_4_Sleep']\n",
    "\n",
    "sleep_act_cluster = ['State_Prob_1_Sleep', 'State_Prob_2_Sleep', 'State_Prob_3_Sleep', 'State_Prob_4_Sleep', \n",
    "                      'State_Prob_1_Act', 'State_Prob_2_Act', 'State_Prob_3_Act']\n",
    "\n",
    "sleep_rmt = ['total_sleep_time_mean', 'total_sleep_time_sd', 'awake_pct_mean',\n",
    "       'sleep_onset_mean', 'sleep_onset_sd', 'sleep_offset_mean',\n",
    "       'sleep_offset_sd', 'sleep_efficiency_mean', 'sleep_efficiency_sd']\n",
    "\n",
    "sleep_act_rmt = ['total_sleep_time_mean', 'total_sleep_time_sd', 'awake_pct_mean',\n",
    "       'sleep_onset_mean', 'sleep_onset_sd', 'sleep_offset_mean',\n",
    "       'sleep_offset_sd', 'sleep_efficiency_mean', 'sleep_efficiency_sd',\n",
    "                 'Sedentary_time_mean', 'Light_activity_mean', 'Moderate_activity_mean', 'Vigorous_activity_mean', \n",
    "       'Nighttime_activity_mean', 'Total_Daily_Calories_mean', 'Sedentary_time_sd', 'Light_activity_sd', \n",
    "       'Moderate_activity_sd', 'Vigorous_activity_sd', 'Nighttime_activity_sd', 'Total_Daily_Calories_sd']\n",
    "\n",
    "#Then define a dictionary with combinations.\n",
    "\n",
    "# to use with df_sleep_pre (SAMPLE 1 - Sleep only)\n",
    "S1_feature_dict = {'chara_only_S1': chara,\n",
    "           'chara_and_sleepcluster': chara + sleep_cluster, \n",
    "           'chara_and_sleepRMT': chara + sleep_rmt, \n",
    "           'chara_and_sleepcluster_sleepRMT': chara + sleep_cluster + sleep_rmt, \n",
    "\n",
    "           'chara_wo_b_ids_S1': chara_wo_baselineids, \n",
    "           'chara_wo_b_ids_and_sleepcluster': chara_wo_baselineids + sleep_cluster, \n",
    "           'chara_wo_b_ids_and_sleepRMT': chara_wo_baselineids + sleep_rmt, \n",
    "           'chara_wo_b_ids_and_sleepcluster_and_sleepRMT': chara_wo_baselineids + sleep_cluster + sleep_rmt,\n",
    "}\n",
    "\n",
    "# to use with df_sleep_act_pre (SAMPLE 2 - Sleep + Activity)\n",
    "S2_feature_dict = {'chara_only_S2': chara,\n",
    "           'chara_and_allCluster': chara + sleep_act_cluster, \n",
    "           'chara_and_allRMT': chara + sleep_act_rmt, \n",
    "           'chara_and_rmt_and_allClusters': chara + sleep_act_cluster + sleep_act_rmt,\n",
    "            \n",
    "           'chara_wo_b_ids_S2': chara_wo_baselineids,\n",
    "           'chara_wo_b_ids_and_allCluster': chara_wo_baselineids + sleep_act_cluster, \n",
    "           'chara_wo_b_ids_and_allRMT': chara_wo_baselineids + sleep_act_rmt, \n",
    "           'chara_wo_b_ids_and_allClusters_and_allRMT': chara_wo_baselineids + sleep_act_cluster + sleep_act_rmt,\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391f5cfa-f224-4c04-b30e-305c25fa30db",
   "metadata": {},
   "source": [
    "#### Selecting the relevant feature dictionary & DATASET "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de45634c-efb7-4848-824c-150659f0a177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_1 or sample_2\n",
    "df = sample_2                                       ####  <- CHANGE SAMPLE HERE \n",
    "\n",
    "# Define pipeline and parameter grid \n",
    "pipeline = Pipeline([\n",
    "    ('imputer', KNNImputer(n_neighbors=5)), \n",
    "    ('scaler', StandardScaler()),\n",
    "    ('randomforest', RandomForestRegressor())\n",
    "])\n",
    "\n",
    "# Define hyperparameters to tune\n",
    "rf_param_grid = {\n",
    "    'randomforest__n_estimators': [10, 25, 50, 100, 250, 500],\n",
    "    'randomforest__max_depth': [5, 10, 25, 50, 100, 250, None],\n",
    "    'randomforest__min_samples_split': [10, 25, 50]\n",
    "}\n",
    "\n",
    "outer = GroupKFold(n_splits=5)\n",
    "inner = GroupKFold(n_splits=5)\n",
    "\n",
    "# Dictionary to store results for each feature set\n",
    "results_dict = {}\n",
    "best_params = {}\n",
    "\n",
    "# Iterate through each feature set in the features dictionary\n",
    "for features, selected_features in S2_feature_dict.items():     ####  <- CHANGE DICT NAME HERE \n",
    "    \n",
    "    # Prepare the independent variables (IVs) and target variable\n",
    "    X = df[selected_features]\n",
    "    y = df['IDS_TOTAL']\n",
    "    groups = df['p_id']\n",
    "        \n",
    "    # Set up inner grid as a gridsearchCV object\n",
    "    grid = GridSearchCV(pipeline,\n",
    "                        rf_param_grid,\n",
    "                        verbose = 1,\n",
    "                        scoring = 'neg_mean_squared_error',\n",
    "                        cv=inner)\n",
    "    \n",
    "    results = cross_validate(grid, X, y, \n",
    "                             cv=outer,\n",
    "                             groups=groups,\n",
    "                             # don't understand this row\n",
    "                             params={'groups': groups}, \n",
    "                             scoring=scorers\n",
    "                             #n_jobs=-1\n",
    "                            )\n",
    "    # Store the results in the dictionary\n",
    "    results_dict[features] = results      \n",
    "    grid.fit(X, y, groups=groups)  # Fit the grid to get best parameters\n",
    "    best_params[features] = grid.best_params_                \n",
    "    \n",
    "    print(f\"Results for {features} stored.\")\n",
    "    print(\"----------------------------------------------\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d64b0b4-befd-42da-973d-efbbb06077ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X), len(y), len(groups))\n",
    "group_id = groups[:len(X)]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb86731-baa5-4669-aac9-15771e146762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to hold the results\n",
    "final_results_list = []\n",
    "\n",
    "# Loop through results_dict and compute the required metrics\n",
    "for features, results in results_dict.items():\n",
    "    final_results_list.append({\n",
    "        'feature_set': features,\n",
    "        'rmse_mean': results['test_rmse'].mean(),\n",
    "        'rmse_sd': results['test_rmse'].std(),\n",
    "        'r2_mean': results['test_r2'].mean(), \n",
    "        'r2_sd': results['test_r2'].std(), \n",
    "        'mae_mean': results['test_mae'].mean(),\n",
    "        'mae_sd': results['test_mae'].std()\n",
    "    })\n",
    "\n",
    "final_results = pd.DataFrame(final_results_list).set_index('feature_set')\n",
    "final_results = final_results.reset_index()\n",
    "display(final_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d0839b-39f7-4da6-accb-9990e179eafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to hold the results\n",
    "final_results_list = []\n",
    "\n",
    "# Loop through results_dict and compute the required metrics\n",
    "for features, results in results_dict.items():\n",
    "    final_results_list.append({\n",
    "        'feature_set': features,\n",
    "        'rmse': f\"{results['test_rmse'].mean():.1f} ({results['test_rmse'].std():.1f})\",\n",
    "        'r2': f\"{results['test_r2'].mean():.1f} ({results['test_r2'].std():.1f})\",\n",
    "        'mae': f\"{results['test_mae'].mean():.1f} ({results['test_mae'].std():.1f})\"\n",
    "    })\n",
    "\n",
    "row_order = [\"chara_only_S2\", \"chara_wo_b_ids_S2\", \n",
    "             \"chara_and_allCluster\", \"chara_wo_b_ids_and_allCluster\", \n",
    "             \"chara_and_allRMT\", \"chara_wo_b_ids_and_allRMT\", \n",
    "             \"chara_and_rmt_and_allClusters\", \"chara_wo_b_ids_and_allClusters_and_allRMT\"]\n",
    "\n",
    "final_results = pd.DataFrame(final_results_list).set_index('feature_set')\n",
    "final_results = final_results.reset_index()\n",
    "\n",
    "# Reorder rows \n",
    "final_results[\"feature_set\"] = pd.Categorical(final_results[\"feature_set\"], categories=row_order, ordered=True)\n",
    "final_results = final_results.sort_values(\"feature_set\").reset_index(drop=True)\n",
    "display(final_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2339dfe0-3d6d-4305-827e-d3d967db1338",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results.set_index(\"feature_set\", inplace=True)\n",
    "df_transposed = final_results.T\n",
    "\n",
    "display(df_transposed)\n",
    "df_transposed = df_transposed.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30372b3-e1b3-40cf-8933-9e863c6c06a6",
   "metadata": {},
   "source": [
    "# Part 2: Feature Importance\n",
    "**Refit to entire dataset (include inner loop - no outer)**\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e2a759fc-2b14-49ba-854f-5dd269c4827a",
   "metadata": {},
   "source": [
    "# to use with sample_2 (SAMPLE 2 - Sleep + Activity)\n",
    "Feature dictionary: S2_feature_dict\n",
    "           'chara_and_rmt_and_allClusters': chara + sleep_act_cluster + sleep_act_rmt,\n",
    "           'chara_wo_b_ids_and_allClusters_and_allRMT': chara_wo_baselineids + sleep_act_cluster + sleep_act_rmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a2bfb9-cfbb-40cf-9f31-fa150eeffe67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the key for the feature set you want to use\n",
    "selected_feature_key = 'chara_and_rmt_and_allClusters'          # <----   CHANGE HERE\n",
    "selected_features = S2_feature_dict[selected_feature_key]         \n",
    "\n",
    "# Prepare the independent variables (IVs) for the regression model\n",
    "X = sample_2[selected_features]    # <----   CHANGE HERE\n",
    "y = sample_2['IDS_TOTAL']          # <----   CHANGE HERE\n",
    "groups = sample_2['p_id']          # <----   CHANGE HERE\n",
    "\n",
    "# Use GroupKFold for inner cross-validation\n",
    "inner = GroupKFold(n_splits=5)\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=rf_param_grid,\n",
    "    scoring = 'neg_mean_squared_error',\n",
    "    cv=inner,\n",
    "    verbose=1\n",
    ")\n",
    " \n",
    "# Refit the grid search on the entire dataset\n",
    "grid_search.fit(X, y, groups=groups)\n",
    "\n",
    "# Get the best model and its coefficients\n",
    "best_model = grid_search.best_estimator_\n",
    "print(best_model)\n",
    "\n",
    "# Access the RandomForest model inside the pipeline\n",
    "randomforest_model = best_model.named_steps['randomforest'] \n",
    "feature_importance = pd.Series(randomforest_model.feature_importances_, index=X.columns)\n",
    "\n",
    "# Sort and display feature importance\n",
    "feature_importance_withbaseline = feature_importance.abs().sort_values(ascending=False)\n",
    "print(feature_importance_withbaseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b842515-a2f1-4589-ae7d-db3e04cc1195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the key for the feature set you want to use\n",
    "selected_feature_key = 'chara_wo_b_ids_and_allClusters_and_allRMT'          # <----   CHANGE HERE\n",
    "selected_features = S2_feature_dict[selected_feature_key]         \n",
    "\n",
    "# Prepare the independent variables (IVs) for the regression model\n",
    "X = sample_2[selected_features]    # <----   CHANGE HERE\n",
    "y = sample_2['IDS_TOTAL']          # <----   CHANGE HERE\n",
    "groups = sample_2['p_id']          # <----   CHANGE HERE\n",
    "\n",
    "# Use GroupKFold for inner cross-validation\n",
    "inner = GroupKFold(n_splits=5)\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=rf_param_grid,\n",
    "    scoring = 'neg_mean_squared_error',\n",
    "    cv=inner,\n",
    "    verbose=1\n",
    ")\n",
    " \n",
    "# Refit the grid search on the entire dataset\n",
    "grid_search.fit(X, y, groups=groups)\n",
    "\n",
    "# Get the best model and its coefficients\n",
    "best_model = grid_search.best_estimator_\n",
    "print(best_model)\n",
    "\n",
    "# Access the RandomForest model inside the pipeline\n",
    "randomforest_model = best_model.named_steps['randomforest'] \n",
    "feature_importance = pd.Series(randomforest_model.feature_importances_, index=X.columns)\n",
    "\n",
    "# Sort and display feature importance\n",
    "feature_importance_NObaseline = feature_importance.abs().sort_values(ascending=False)\n",
    "print(feature_importance_NObaseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adb0462-e1a7-43c4-a147-fead31804427",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
