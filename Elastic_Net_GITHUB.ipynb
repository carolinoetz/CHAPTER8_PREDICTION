{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c60a4e02-1192-4160-8947-0dd060766175",
   "metadata": {},
   "source": [
    "# Elastic Net on HMM Sleep Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1572d2c0-ada2-4903-b273-4dd27f09c703",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate, GridSearchCV, train_test_split, GroupKFold\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, r2_score, mean_absolute_error, explained_variance_score\n",
    "from sklearn.linear_model import ElasticNet, LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import KNNImputer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.model_selection import GridSearchCV, GroupKFold\n",
    "from math import sqrt\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138cb6ff-04fe-4748-9ea2-eefa4518c1c3",
   "metadata": {},
   "source": [
    "## Functions\n",
    "**Create scorer's for the models to assess their fit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5fe688-a5c8-40ac-9dcc-d643a3c3a955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom RMSE scorer\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "# Custom calibration slope scorer\n",
    "def calibration_slope(y_true, y_pred):\n",
    "    model = LinearRegression()\n",
    "    model.fit(y_pred.reshape(-1, 1), y_true)\n",
    "    return model.coef_[0]\n",
    "\n",
    "# Custom calibration intercept scorer\n",
    "def calibration_intercept(y_true, y_pred):\n",
    "    model = LinearRegression()\n",
    "    model.fit(y_pred.reshape(-1, 1), y_true)\n",
    "    return model.intercept_\n",
    "\n",
    "# Define the scorer's dictionary\n",
    "scorers = {\n",
    "    'rmse': make_scorer(rmse),\n",
    "    'r2': make_scorer(r2_score),\n",
    "    'mae': make_scorer(mean_absolute_error),\n",
    "    'explained_variance': make_scorer(explained_variance_score),\n",
    "    'calibration_slope': make_scorer(calibration_slope),\n",
    "    'calibration_intercept': make_scorer(calibration_intercept)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8837af8d-c9da-47db-9dc9-e8804c7f1fc6",
   "metadata": {},
   "source": [
    "## Dataset up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db7a947-b1e3-4f05-83e6-d5b7dd957284",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sleep = pd.read_csv(\"Sleep_total_data_scaled_probs.csv\")\n",
    "df_activity = pd.read_csv(\"Act_total_data_scaled_probs.csv\")\n",
    "df_predictors = pd.read_csv(\"df_predictors.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fac490-6bb5-4b81-9839-ee3bc821d4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SLEEP!!!! \n",
    "df_sleep = df_sleep.rename(columns={'hmm': 'HMM_State_Sleep', \n",
    "                                    'State_Prob_1': 'State_Prob_1_Sleep',\n",
    "                                    'State_Prob_2': 'State_Prob_2_Sleep',\n",
    "                                    'State_Prob_3': 'State_Prob_3_Sleep', \n",
    "                                    'State_Prob_4': 'State_Prob_4_Sleep'})\n",
    "\n",
    "df_sleep = df_sleep[['p_id','timepoint', 'IDS_TOTAL','HMM_State_Sleep','State_Prob_1_Sleep', 'State_Prob_2_Sleep',\n",
    "       'State_Prob_3_Sleep','State_Prob_4_Sleep', 'sleep_day', \n",
    "       'total_sleep_time_mean', 'total_sleep_time_sd', 'awake_pct_mean',\n",
    "       'sleep_onset_mean', 'sleep_onset_sd', 'sleep_offset_mean',\n",
    "       'sleep_offset_sd', 'sleep_efficiency_mean', 'sleep_efficiency_sd']]\n",
    "\n",
    "\n",
    "# ACTIVITY !!!!\n",
    "df_activity = df_activity.rename(columns={'hmm': 'HMM_State_Act', \n",
    "                                    'State_Prob_1': 'State_Prob_1_Act',\n",
    "                                    'State_Prob_2': 'State_Prob_2_Act',\n",
    "                                    'State_Prob_3': 'State_Prob_3_Act'})\n",
    "\n",
    "df_activity = df_activity[['p_id','timepoint','HMM_State_Act', 'State_Prob_1_Act', 'State_Prob_2_Act',\n",
    "                           'State_Prob_3_Act','act_day',\n",
    "                           'Sedentary_time_mean',\n",
    "                           'Light_activity_mean', 'Moderate_activity_mean',\n",
    "                           'Vigorous_activity_mean', 'Nighttime_activity_mean',\n",
    "                           'Total_Daily_Calories_mean', 'Sedentary_time_sd', 'Light_activity_sd',\n",
    "                           'Moderate_activity_sd', 'Vigorous_activity_sd', 'Nighttime_activity_sd',\n",
    "                           'Total_Daily_Calories_sd']]              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bbdfd5-9b5a-4c13-876d-eb98a2df6ba0",
   "metadata": {},
   "source": [
    "**Sleep + Baseline (predictors) DATASET**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9c5fc5-78a9-4ca9-baf4-ca0372cd8ce4",
   "metadata": {},
   "source": [
    "Gender: Female = 0; Male =1 \n",
    "Deciding against using dummy variables & using the probabilites instead -- if I was to use the cluster memberships again MAKE SURE TO change the code to not include NAs as zeros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfa2345-c4da-4525-9f5d-0f4cf740f506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge sleep & act data \n",
    "df_sleep_pre = pd.merge(df_sleep, df_predictors, on = ['p_id', 'timepoint'], how= 'inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa64d41-3d5a-4a84-9e6f-c8832f032c14",
   "metadata": {},
   "source": [
    "**Sleep + Activity + Baseline (Predictor) DATASET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f04daac-89ca-4c24-a683-fe68616a5cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sleep_act = pd.merge(df_activity, df_sleep, on = ['p_id', 'timepoint'], how= 'inner')\n",
    "df_sleep_act_pre = pd.merge(df_sleep_act, df_predictors, on = ['p_id', 'timepoint'], how= 'inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e9f8f1-166c-4156-b165-845f3b1e6189",
   "metadata": {},
   "source": [
    "**Remove rows where timepoint == 0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418eeb6d-1336-40d6-a425-4c0accdbb437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where 'Score' equals 0\n",
    "df_sleep_pre = df_sleep_pre[df_sleep_pre['timepoint'] != 0]\n",
    "df_sleep_act_pre = df_sleep_act_pre[df_sleep_act_pre['timepoint'] != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2102171-71b9-4ed5-87db-96c598f7a77d",
   "metadata": {},
   "source": [
    "**Explore missing data in either sample**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc89ee23-8d95-48c5-a002-78e23e159f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the display option to show all rows\n",
    "pd.set_option('display.max_rows', None)\n",
    "missing_values_df = df_sleep_pre.isna().sum().reset_index()    # <- CHANGE HERE\n",
    "non_missing_values_df = df_sleep_pre.notna().sum().reset_index(drop=True)    # <- CHANGE HERE\n",
    "\n",
    "na_df = missing_values_df\n",
    "na_df.columns = ['Variable', 'Missing Values']\n",
    "na_df['Non-missing Values'] = non_missing_values_df\n",
    "print(na_df)\n",
    "\n",
    "pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b81358-2b20-4622-8e2f-4650838d5fe8",
   "metadata": {},
   "source": [
    "### The two final samples \n",
    "**Sample 1 = sleep only**\n",
    "\n",
    "**Sample 2 = sleep + activity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6183947d-36e0-40b7-8e9f-f9016f24be0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_1 = df_sleep_pre\n",
    "sample_2 = df_sleep_act_pre\n",
    "\n",
    "num_unique_participants = sample_1['p_id'].nunique()\n",
    "print(\"Number of unique - SAMPLE 1:\", num_unique_participants)\n",
    "\n",
    "num_unique_participants = sample_2['p_id'].nunique()\n",
    "print(\"Number of unique - SAMPLE 2:\", num_unique_participants)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a7b3ec-e277-4446-a461-0ae56a332271",
   "metadata": {},
   "source": [
    "# Part 1: Elastic Net - Nested CV "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92812a7e-4930-4c9f-b27a-486b4cf4d9ed",
   "metadata": {},
   "source": [
    "Here we have an outer and an inner loop. The outer loop is used to evaluate the model & the inner loop to tune the hyperparameters. \n",
    "1. Outer Loop: Use GroupKFold for cross-validation (same as above)\n",
    "2. Inner Loop: Use GridSearchCV within each of these folds from the outer loop to tune the hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d17d7fd-5e3b-4354-8708-2cb0ee34c1a2",
   "metadata": {},
   "source": [
    "#### Creating feature set dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e263788-9ef0-4c0c-9096-e0f44e35bf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chara = ['Baseline_IDS_TOTAL',\n",
    "            'age_all', 'gender_num', 'mh_family_depr___0', 'mh_family_depr___1', 'mh_family_depr___2', 'LIFETIME_TRAUMA',\n",
    "            'Mental_comorbidity', 'Physical_comorbidity'\n",
    "           ]\n",
    "\n",
    "chara_wo_baselineids = [\n",
    "            'age_all', 'gender_num', 'mh_family_depr___0', 'mh_family_depr___1', 'mh_family_depr___2', 'LIFETIME_TRAUMA',\n",
    "            'Mental_comorbidity', 'Physical_comorbidity'\n",
    "           ]\n",
    "\n",
    "sleep_cluster = ['State_Prob_1_Sleep', 'State_Prob_2_Sleep', 'State_Prob_3_Sleep', 'State_Prob_4_Sleep']\n",
    "\n",
    "sleep_act_cluster = ['State_Prob_1_Sleep', 'State_Prob_2_Sleep', 'State_Prob_3_Sleep', 'State_Prob_4_Sleep', \n",
    "                      'State_Prob_1_Act', 'State_Prob_2_Act', 'State_Prob_3_Act']\n",
    "\n",
    "sleep_rmt = ['total_sleep_time_mean', 'total_sleep_time_sd', 'awake_pct_mean',\n",
    "       'sleep_onset_mean', 'sleep_onset_sd', 'sleep_offset_mean',\n",
    "       'sleep_offset_sd', 'sleep_efficiency_mean', 'sleep_efficiency_sd']\n",
    "\n",
    "sleep_act_rmt = ['total_sleep_time_mean', 'total_sleep_time_sd', 'awake_pct_mean',\n",
    "       'sleep_onset_mean', 'sleep_onset_sd', 'sleep_offset_mean',\n",
    "       'sleep_offset_sd', 'sleep_efficiency_mean', 'sleep_efficiency_sd',\n",
    "                 'Sedentary_time_mean', 'Light_activity_mean', 'Moderate_activity_mean', 'Vigorous_activity_mean', \n",
    "       'Nighttime_activity_mean', 'Total_Daily_Calories_mean', 'Sedentary_time_sd', 'Light_activity_sd', \n",
    "       'Moderate_activity_sd', 'Vigorous_activity_sd', 'Nighttime_activity_sd', 'Total_Daily_Calories_sd']\n",
    "\n",
    "#Then define a dictionary with combinations.\n",
    "\n",
    "# to use with df_sleep_pre (SAMPLE 1 - Sleep only)\n",
    "S1_feature_dict = {'chara_only_S1': chara,\n",
    "           'chara_and_sleepcluster': chara + sleep_cluster, \n",
    "           'chara_and_sleepRMT': chara + sleep_rmt, \n",
    "           'chara_and_sleepcluster_sleepRMT': chara + sleep_cluster + sleep_rmt, \n",
    "\n",
    "           'chara_wo_b_ids_S1': chara_wo_baselineids, \n",
    "           'chara_wo_b_ids_and_sleepcluster': chara_wo_baselineids + sleep_cluster, \n",
    "           'chara_wo_b_ids_and_sleepRMT': chara_wo_baselineids + sleep_rmt, \n",
    "           'chara_wo_b_ids_and_sleepcluster_and_sleepRMT': chara_wo_baselineids + sleep_cluster + sleep_rmt,\n",
    "}\n",
    "\n",
    "# to use with df_sleep_act_pre (SAMPLE 2 - Sleep + Activity)\n",
    "S2_feature_dict = {'chara_only_S2': chara,\n",
    "           'chara_and_allCluster': chara + sleep_act_cluster, \n",
    "           'chara_and_allRMT': chara + sleep_act_rmt, \n",
    "           'chara_and_rmt_and_allClusters': chara + sleep_act_cluster + sleep_act_rmt,\n",
    "            \n",
    "           'chara_wo_b_ids_S2': chara_wo_baselineids,\n",
    "           'chara_wo_b_ids_and_allCluster': chara_wo_baselineids + sleep_act_cluster, \n",
    "           'chara_wo_b_ids_and_allRMT': chara_wo_baselineids + sleep_act_rmt, \n",
    "           'chara_wo_b_ids_and_allClusters_and_allRMT': chara_wo_baselineids + sleep_act_cluster + sleep_act_rmt,\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391f5cfa-f224-4c04-b30e-305c25fa30db",
   "metadata": {},
   "source": [
    "#### Selecting the relevant feature dictionary & DATASET "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1a568e-7b85-46f5-bebc-5771e9dc4e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_1 or sample_2\n",
    "df = sample_1                                       ####  <- CHANGE SAMPLE HERE \n",
    "\n",
    "# Define pipeline and parameter grid \n",
    "pipeline = Pipeline([\n",
    "    ('imputer', KNNImputer(n_neighbors=5)), \n",
    "    ('scaler', StandardScaler()),\n",
    "    ('elasticnet', ElasticNet())\n",
    "])\n",
    "\n",
    "# Define hyperparameters to search\n",
    "param_grid = {\n",
    "    'elasticnet__alpha': [0.01, 0.1, 1.0, 10.0, 100],\n",
    "    'elasticnet__l1_ratio': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "}\n",
    "\n",
    "outer = GroupKFold(n_splits=5)\n",
    "inner = GroupKFold(n_splits=5)\n",
    "\n",
    "# Dictionary to store results for each feature set\n",
    "results_dict = {}\n",
    "best_params = {}\n",
    "\n",
    "# Iterate through each feature set in the features dictionary\n",
    "for features, selected_features in S1_feature_dict.items():     ####  <- CHANGE DICT NAME HERE \n",
    "    \n",
    "    # Prepare the independent variables (IVs) and target variable\n",
    "    X = df[selected_features]\n",
    "    y = df[['IDS_TOTAL']]\n",
    "    groups = df['p_id']\n",
    "        \n",
    "    # Set up inner grid as a gridsearchCV object\n",
    "    grid = GridSearchCV(pipeline,\n",
    "                        param_grid,\n",
    "                        verbose = 1,\n",
    "                        scoring = 'neg_mean_squared_error',\n",
    "                        cv=inner)\n",
    "    \n",
    "    results = cross_validate(grid, X, y, \n",
    "                             cv=outer,\n",
    "                             groups=groups,\n",
    "                             # don't understand this row\n",
    "                             params={'groups': groups}, \n",
    "                             scoring=scorers\n",
    "                             #n_jobs=-1\n",
    "                            )\n",
    "    # Store the results in the dictionary\n",
    "    results_dict[features] = results      \n",
    "    grid.fit(X, y, groups=groups)  # Fit the grid to get best parameters\n",
    "    best_params[features] = grid.best_params_                \n",
    "    \n",
    "    print(f\"Results for {features} stored.\")\n",
    "    print(\"----------------------------------------------\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d64b0b4-befd-42da-973d-efbbb06077ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X), len(y), len(groups))\n",
    "group_id = groups[:len(X)]  # Ensure group_id matches length of X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b338ae-45ed-4e9a-a906-35beb35216a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to hold the results\n",
    "final_results_list = []\n",
    "\n",
    "# Loop through results_dict and compute the required metrics\n",
    "for features, results in results_dict.items():\n",
    "    final_results_list.append({\n",
    "        'feature_set': features,\n",
    "        'rmse': f\"{results['test_rmse'].mean():.1f} ({results['test_rmse'].std():.1f})\",\n",
    "        'r2': f\"{results['test_r2'].mean():.1f} ({results['test_r2'].std():.1f})\",\n",
    "        'mae': f\"{results['test_mae'].mean():.1f} ({results['test_mae'].std():.1f})\"\n",
    "    })\n",
    "\n",
    "row_order = [\"chara_only_S1\", \"chara_wo_b_ids_S1\", \n",
    "             \"chara_and_sleepcluster\", \"chara_wo_b_ids_and_sleepcluster\", \n",
    "             \"chara_and_sleepRMT\", \"chara_wo_b_ids_and_sleepRMT\", \n",
    "             \"chara_and_sleepcluster_sleepRMT\", \"chara_wo_b_ids_and_sleepcluster_and_sleepRMT\"]\n",
    "\n",
    "\n",
    "final_results = pd.DataFrame(final_results_list).set_index('feature_set')\n",
    "final_results = final_results.reset_index()\n",
    "\n",
    "# Reorder rows \n",
    "final_results[\"feature_set\"] = pd.Categorical(final_results[\"feature_set\"], categories=row_order, ordered=True)\n",
    "final_results = final_results.sort_values(\"feature_set\").reset_index(drop=True)\n",
    "display(final_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212f16df-0e51-41c1-8583-c62bed90eef3",
   "metadata": {},
   "source": [
    "# Part 2: Feature Importance\n",
    "**Refit to entire dataset (include inner loop - no outer)**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ba2a332c-f7c0-4399-84ce-7a0ad99ddc8a",
   "metadata": {},
   "source": [
    "#### to use with sample_1 (SAMPLE 1 - Sleep only)\n",
    "'chara_and_sleepcluster_sleepRMT': chara + sleep_cluster + sleep_rmt, \n",
    "'chara_wo_b_ids_and_sleepcluster_and_sleepRMT': chara_wo_baselineids + sleep_cluster + sleep_rmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c47c1c4-c30f-49f7-ae67-909bff36c745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the key for the feature set you want to use\n",
    "selected_feature_key = 'chara_and_sleepcluster_sleepRMT'          # <----   CHANGE HERE\n",
    "selected_features = S1_feature_dict[selected_feature_key]         \n",
    " \n",
    "# Prepare the independent variables (IVs) for the regression model\n",
    "X = sample_1[selected_features]    # <----   CHANGE HERE\n",
    "y = sample_1[['IDS_TOTAL']]        # <----   CHANGE HERE\n",
    "groups = sample_1['p_id']          # <----   CHANGE HERE\n",
    "\n",
    "# Use GroupKFold for inner cross-validation\n",
    "inner = GroupKFold(n_splits=5)\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring = 'neg_mean_squared_error',\n",
    "    cv=inner,\n",
    "    verbose=1,\n",
    ")\n",
    " \n",
    "# Refit the grid search on the entire dataset\n",
    "grid_search.fit(X, y, groups=groups)\n",
    "\n",
    "# Get the best model and its coefficients\n",
    "best_model = grid_search.best_estimator_\n",
    "print(best_model)\n",
    "best_params = grid_search.best_params_ \n",
    "print(best_params)\n",
    "\n",
    "# Access the elastic net coefficients\n",
    "elasticnet = best_model.named_steps['elasticnet']\n",
    "feature_importance = pd.Series(elasticnet.coef_, index=X.columns)\n",
    "\n",
    "# Sort and display feature importance\n",
    "feature_importance = feature_importance.sort_values(ascending=False)\n",
    "abs_feature_importance = feature_importance.abs().sort_values(ascending=False)\n",
    "print(abs_feature_importance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef985638-f533-44cb-a457-211a6162f184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the key for the feature set you want to use\n",
    "selected_feature_key = 'chara_wo_b_ids_and_sleepcluster_and_sleepRMT'          # <----   CHANGE HERE\n",
    "selected_features = S1_feature_dict[selected_feature_key]         \n",
    " \n",
    "# Prepare the independent variables (IVs) for the regression model\n",
    "X = sample_1[selected_features]    # <----   CHANGE HERE\n",
    "y = sample_1[['IDS_TOTAL']]        # <----   CHANGE HERE\n",
    "groups = sample_1['p_id']          # <----   CHANGE HERE\n",
    "\n",
    "# Use GroupKFold for inner cross-validation\n",
    "inner = GroupKFold(n_splits=5)\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring = 'neg_mean_squared_error',\n",
    "    cv=inner,\n",
    "    verbose=1,\n",
    ")\n",
    " \n",
    "# Refit the grid search on the entire dataset\n",
    "grid_search.fit(X, y, groups=groups)\n",
    "\n",
    "# Get the best model and its coefficients\n",
    "best_model = grid_search.best_estimator_\n",
    "print(best_model)\n",
    "best_params = grid_search.best_params_ \n",
    "print(best_params)\n",
    "\n",
    "# Access the elastic net coefficients\n",
    "elasticnet = best_model.named_steps['elasticnet']\n",
    "feature_importance = pd.Series(elasticnet.coef_, index=X.columns)\n",
    "\n",
    "# Sort and display feature importance\n",
    "feature_importance_No = feature_importance.sort_values(ascending=False)\n",
    "abs_feature_importance_No = feature_importance.abs().sort_values(ascending=False)\n",
    "print(abs_feature_importance_No)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941d0dbd-035f-4032-9b15-deb9c668941d",
   "metadata": {},
   "source": [
    "## End to current code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
